[{
	"title": "Assignment-2",
	"activityType": "assignment",
	"questions": [{
		"questionType": "fillintheblank",
		"max_marks": 40,
		"graded": true,
		"questionText": [{
				"text": "<b>The below is the continuation of yesterday's problem so once go through the previous days' problem and start today's problem<br>Every day you will be given a code you need to trace it and record a screencast of the tracing and submit it. Reach your mentor for the exact format of the video</b>"
			},
			{
				"text": "<br><br><br><b>Search</b><br><br>The other route is to search for a solution: to systematically try all possibilities until we hit one that works. The code for this is less than a dozen lines, but we run another risk: that it might take forever to run. Consider that in the grid2 above, A2 has 4 possibilities (1679) and A3 has 5 possibilities (12679); together that's 20, and if we keep multiplying, we get 4.62838344192 × 1038 possibilities for the whole puzzle. How can we cope with that? There are two choices.<br><br>First, we could try a brute force approach. Suppose we have a very efficient program that takes only one instruction to evaluate a position, and that we have access to the next-generation computing technology, let's say a 10GHz processor with 1024 cores, and let's say we could afford a million of them, and while we're shopping, let's say we also pick up a time machine and go back 13 billion years to the origin of the universe and start our program running. We can then compute that we'd be almost 1% done with this one puzzle by now.<br><br>The second choice is to somehow process much more than one possibility per machine instruction. That seems impossible, but fortunately, it is exactly what constraint propagation does for us. We don't have to try all 4 × 1038 possibilities because as soon as we try one we immediately eliminate many other possibilities. For example, square H7 of this puzzle has two possibilities, 6 and 9. We can try 9 and quickly see that there is a contradiction. That means we've eliminated not just one possibility, but fully half of the 4 × 1038 choices.<br><br>In fact, it turns out that to solve this particular puzzle we need to look at only 25 possibilities and we only have to explicitly search through 9 of the 61 unfilled squares; constraint propagation does the rest. For the list of 95 <a href=\"http://magictour.free.fr/top95\"> hard puzzles</a> on average we need to consider 64 possibilities per puzzle, and in no case do we have to search more than 16 squares.<br><br>What is the search algorithm? Simple: first make sure we haven't already found a solution or a contradiction, and if not, choose one unfilled square and consider all its possible values. One at a time, try assigning the square each value, and searching from the resulting position. In other words, we search for a value d such that we can successfully search for a solution from the result of assigning square s to d. If the search leads to an failed position, go back and consider another value of d. This is a recursive search, and we call it a depth-first search because we (recursively) consider all possibilities under values[s] = d before we consider a different value for s.<br><br>To avoid bookkeeping complications, we create a new copy of values for each recursive call to search. This way each branch of the search tree is independent and doesn't confuse another branch. (This is why I chose to implement the set of possible values for a square as a string: I can copy values with values.copy() which is simple and efficient. If I implemented a possibility as a Python set or list I would need to use the copy. deep copy(values), which is less efficient.) The alternative is to keep track of each change to values and undo the change when we hit a dead end. This is known as backtracking search. It makes sense when each step in the search is a single change to a large data structure but is complicated when each assignment can lead to many other changes via constraint propagation.<br><br>There are two choices we have to make in implementing the search: variable ordering (which square do we try first?) and value ordering (which digit do we try first for the square?). For variable ordering, we will use a common heuristic called minimum remaining values, which means that we choose the (or one of the) square with the minimum number of possible values. Why? Consider grid2 above. Suppose we chose B3 first. It has 7 possibilities (1256789), so we'd expect to guess wrong with probability 6/7. If instead, we chose G2, which only has 2 possibilities (89), we'd expect to be wrong with probability only 1/2. Thus we choose the square with the fewest possibilities and the best chance of guessing right. For value ordering we won't do anything special; we'll consider the digits in numeric order.<br><br>Now we're ready to define the solve function in terms of the search function"
			},
			{
				"image": {
					"imageName": "",
					"imageSRC": "image-1.png"
				}
			},
			{
				"text": "Now before we conclude, we will need to display a puzzle:"
			},
			{
				"image": {
					"imageName": "",
					"imageSRC": "image-2.png"
				}
			},
			{
				"text": "That's it! We're done; it only took one page of code, and we can now solve any Sudoku puzzle in the following way"
			},
			{
				"image": {
					"imageName": "",
					"imageSRC": "image-3.png"
				}
			},
			{
				"image": {
					"imageName": "",
					"imageSRC": "image-4.png"
				}
			}
		]
	}]
}]